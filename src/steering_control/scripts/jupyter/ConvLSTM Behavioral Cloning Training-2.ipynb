{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import BatchGenerator\n",
    "import utils\n",
    "# from SamplingRNN import SamplingRNNCell\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configeration\n",
    "\n",
    "These are some very basic parameters for constructing training the model. In the future, it would be very helper to test different parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 10 # this parameter can be changed. TODO: try longer sequences if memory is available.\n",
    "BATCH_SIZE = 4 # this parameter can also be changed\n",
    "LEFT_CONTEXT = 5\n",
    "\n",
    "HEIGHT = 480\n",
    "WIDTH = 640\n",
    "CHANNELS = 3\n",
    "\n",
    "RNN_SIZE = 32\n",
    "RNN_PROJ = 32\n",
    "\n",
    "CSV_HEADER = \"index, timestamp, width, height, frame_id, filename, angle, torque, speed, lat, long, alt\".split(\",\")\n",
    "OUTPUTS = CSV_HEADER[-6:-3] # angle,torque,speed\n",
    "OUTPUT_DIM = len(OUTPUTS) # predict all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/YongyangNie/Desktop/csvs/main.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-08a1023c2257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mTEST_DS_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/YongyangNie/Desktop/csvs/test.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_DS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# concatenated interpolated.csv from rosbags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# interpolated.csv for testset filled with dummy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/neil/Workspace/self-driving-golf-cart/src/steering_control/scripts/jupyter/utils.pyc\u001b[0m in \u001b[0;36mprocess_csv\u001b[0;34m(filename, val)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0msum_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat128\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0msum_sq_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat128\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;31m# leave val% for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtrain_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/neil/Workspace/self-driving-golf-cart/src/steering_control/scripts/jupyter/utils.pyc\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mln\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# imagefile, outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/YongyangNie/Desktop/csvs/main.csv'"
     ]
    }
   ],
   "source": [
    "TRAIN_DS_PATH = \"/home/neil/dataset/udacity/main.csv\"\n",
    "TEST_DS_PATH = \"/home/neil/dataset/udacity/test.csv\"\n",
    "\n",
    "(train_seq, valid_seq), (mean, std) = utils.process_csv(filename=TRAIN_DS_PATH, val=5)  # concatenated interpolated.csv from rosbags\n",
    "\n",
    "# interpolated.csv for testset filled with dummy values\n",
    "test_seq = utils.read_csv(TEST_DS_PATH)\n",
    "\n",
    "print(len(list(test_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = lambda x: tf.contrib.layers.layer_norm(inputs=x, center=True, scale=True, activation_fn=None, trainable=True)\n",
    "\n",
    "def apply_vision_simple(image, keep_prob, batch_size, seq_len, scope=None, reuse=None):\n",
    "    video = tf.reshape(image, shape=[batch_size, LEFT_CONTEXT + seq_len, HEIGHT, WIDTH, CHANNELS])\n",
    "    with tf.variable_scope(scope, 'Vision', [image], reuse=reuse):\n",
    "            net = slim.convolution(video, num_outputs=64, kernel_size=[3,12,12], stride=[1,6,6], padding=\"VALID\")\n",
    "            net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "            aux1 = slim.fully_connected(tf.reshape(net[:, -seq_len:, :, :, :], [batch_size, seq_len, -1]), 128, activation_fn=None)\n",
    "            net = slim.convolution(net, num_outputs=64, kernel_size=[2,5,5], stride=[1,2,2], padding=\"VALID\")\n",
    "            net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "            aux2 = slim.fully_connected(tf.reshape(net[:, -seq_len:, :, :, :], [batch_size, seq_len, -1]), 128, activation_fn=None)\n",
    "            net = slim.convolution(net, num_outputs=64, kernel_size=[2,5,5], stride=[1,1,1], padding=\"VALID\")\n",
    "            net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "            aux3 = slim.fully_connected(tf.reshape(net[:, -seq_len:, :, :, :], [batch_size, seq_len, -1]), 128, activation_fn=None)\n",
    "            net = slim.convolution(net, num_outputs=64, kernel_size=[2,5,5], stride=[1,1,1], padding=\"VALID\")\n",
    "            net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "            print(net) # TODO must be batch_size x seq_len x ...\n",
    "            aux4 = slim.fully_connected(tf.reshape(net, [batch_size, seq_len, -1]), 128, activation_fn=None)\n",
    "            net = slim.fully_connected(tf.reshape(net, [batch_size, seq_len, -1]), 1024, activation_fn=tf.nn.relu)\n",
    "            net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "            net = slim.fully_connected(net, 512, activation_fn=tf.nn.relu)\n",
    "            net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "            net = slim.fully_connected(net, 256, activation_fn=tf.nn.relu)\n",
    "            net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "            net = slim.fully_connected(net, 128, activation_fn=None)\n",
    "            return layer_norm(tf.nn.elu(net + aux1 + aux2 + aux3 + aux4))\n",
    "        \n",
    "        \n",
    "class SamplingRNNCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \n",
    "    \"\"\"Simple sampling RNN cell.\"\"\"\n",
    "    def __init__(self, num_outputs, use_ground_truth, internal_cell, keep_prob):\n",
    "        \"\"\"\n",
    "        if use_ground_truth then don't sample\n",
    "        \"\"\"\n",
    "        self._num_outputs = num_outputs\n",
    "        self._use_ground_truth = use_ground_truth\n",
    "        self._internal_cell = internal_cell\n",
    "        self._keep_prob = keep_prob\n",
    "  \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_outputs, self._internal_cell.state_size # previous output and bottleneck state\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_outputs\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        (visual_feats, current_ground_truth) = inputs\n",
    "        prev_output, prev_state_internal = state\n",
    "        # the following is just for a baseline\n",
    "        context = tf.concat([prev_output, visual_feats], 1)\n",
    "        new_output_internal, new_state_internal = internal_cell(context, prev_state_internal)\n",
    "        new_output = tf.contrib.layers.fully_connected(inputs=tf.concat([new_output_internal, prev_output, visual_feats], 1), \n",
    "                                                       num_outputs=self._num_outputs, activation_fn=None, scope=\"OutputProjection\")\n",
    "        return new_output, (current_ground_truth if self._use_ground_truth else new_output, new_state_internal)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # inputs\n",
    "    learning_rate = tf.placeholder_with_default(input=1e-4, shape=())\n",
    "    keep_prob = tf.placeholder_with_default(input=1.0, shape=())\n",
    "\n",
    "    inputs = tf.placeholder(shape=(BATCH_SIZE, LEFT_CONTEXT + SEQ_LEN),\n",
    "                                    dtype=tf.string)  # pathes to png files from the central camera\n",
    "    targets = tf.placeholder(shape=(BATCH_SIZE, SEQ_LEN, OUTPUT_DIM),\n",
    "                                     dtype=tf.float32)  # seq_len x batch_size x OUTPUT_DIM\n",
    "    targets_normalized = (targets - mean) / std\n",
    "\n",
    "    input_images = tf.stack([tf.image.decode_png(tf.read_file(x)) for x in \n",
    "                             tf.unstack(tf.reshape(inputs, shape=[(LEFT_CONTEXT + SEQ_LEN) * BATCH_SIZE]))])\n",
    "    input_images = -1.0 + 2.0 * tf.cast(input_images, tf.float32) / 255.0\n",
    "    input_images.set_shape([(LEFT_CONTEXT + SEQ_LEN) * BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])\n",
    "    visual_conditions_reshaped = apply_vision_simple(image=input_images, keep_prob=keep_prob, \n",
    "                                                     batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n",
    "    visual_conditions = tf.reshape(visual_conditions_reshaped, [BATCH_SIZE, SEQ_LEN, -1])\n",
    "    visual_conditions = tf.nn.dropout(x=visual_conditions, keep_prob=keep_prob)\n",
    "\n",
    "    rnn_inputs_with_ground_truth = (visual_conditions, targets_normalized)\n",
    "    rnn_inputs_autoregressive = (visual_conditions, tf.zeros(shape=(BATCH_SIZE, SEQ_LEN, OUTPUT_DIM), dtype=tf.float32))\n",
    "\n",
    "    internal_cell = tf.nn.rnn_cell.LSTMCell(num_units=RNN_SIZE, num_proj=RNN_PROJ)\n",
    "    cell_with_ground_truth = SamplingRNNCell(num_outputs=OUTPUT_DIM, use_ground_truth=True,\n",
    "                                             internal_cell=internal_cell, keep_prob=keep_prob)\n",
    "    cell_autoregressive = SamplingRNNCell(num_outputs=OUTPUT_DIM, use_ground_truth=False,\n",
    "                                          internal_cell=internal_cell, keep_prob=keep_prob)\n",
    "\n",
    "    controller_initial_state_variables = utils.get_initial_state(cell_autoregressive.state_size)\n",
    "    controller_initial_state_autoregressive = utils.deep_copy_initial_state(controller_initial_state_variables)\n",
    "    controller_initial_state_gt = utils.deep_copy_initial_state(controller_initial_state_variables)\n",
    "\n",
    "    with tf.variable_scope(\"predictor\"):\n",
    "        out_gt, controller_final_state_gt = tf.nn.dynamic_rnn(cell=cell_with_ground_truth,\n",
    "                                                                      inputs=rnn_inputs_with_ground_truth,\n",
    "                                                                      sequence_length=[SEQ_LEN] * BATCH_SIZE,\n",
    "                                                                      initial_state=controller_initial_state_gt,\n",
    "                                                                      dtype=tf.float32,\n",
    "                                                                      swap_memory=True, time_major=False)\n",
    "    with tf.variable_scope(\"predictor\", reuse=True):\n",
    "        out_autoregressive, controller_final_state_autoregressive = tf.nn.dynamic_rnn(cell=cell_autoregressive, \n",
    "                                                                                      inputs=rnn_inputs_autoregressive,\n",
    "                                                                                      sequence_length=[SEQ_LEN] * BATCH_SIZE,\n",
    "                                                                                      initial_state=controller_initial_state_autoregressive, \n",
    "                                                                                      dtype=tf.float32, \n",
    "                                                                                      swap_memory=True,\n",
    "                                                                                      time_major=False)\n",
    "\n",
    "    mse_gt = tf.reduce_mean(tf.squared_difference(out_gt, targets_normalized))\n",
    "    mse_autoregressive = tf.reduce_mean(tf.squared_difference(out_autoregressive, targets_normalized))\n",
    "    mse_autoregressive_steering = tf.reduce_mean(tf.squared_difference(out_autoregressive[:, :, 0], targets_normalized[:, :, 0]))\n",
    "    steering_predictions = (out_autoregressive[:, :, 0] * std[0]) + mean[0]\n",
    "\n",
    "    total_loss = mse_autoregressive_steering  # + 0.1 * (mse_gt + mse_autoregressive)\n",
    "\n",
    "    optimizer = utils.get_optimizer(total_loss, learning_rate)\n",
    "\n",
    "    tf.summary.scalar(\"MAIN TRAIN METRIC: rmse_autoregressive_steering\", tf.sqrt(mse_autoregressive_steering))\n",
    "    tf.summary.scalar(\"rmse_gt\", tf.sqrt(mse_gt))\n",
    "    tf.summary.scalar(\"rmse_autoregressive\", tf.sqrt(mse_autoregressive))\n",
    "\n",
    "    summaries = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('v3/train_summary', graph=graph)\n",
    "    valid_writer = tf.summary.FileWriter('v3/valid_summary', graph=graph)\n",
    "    saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "\n",
    "checkpoint_dir = os.getcwd() + \"/v3\"\n",
    "\n",
    "global_train_step = 0\n",
    "global_valid_step = 0\n",
    "global_valid_predictions = {}\n",
    "\n",
    "KEEP_PROB_TRAIN = 0.25\n",
    "\n",
    "def do_epoch(session, sequences, mode):\n",
    "    \n",
    "    global global_train_step, global_valid_step, global_valid_predictions\n",
    "    test_predictions = {}\n",
    "    batch_generator = BatchGenerator(sequence=sequences, seq_len=SEQ_LEN, batch_size=BATCH_SIZE)\n",
    "    total_num_steps = int(1 + (batch_generator.indices[1] - 1) / SEQ_LEN)\n",
    "    controller_final_state_gt_cur, controller_final_state_autoregressive_cur = None, None\n",
    "    acc_loss = np.float128(0.0)\n",
    "    \n",
    "    for step in range(total_num_steps):\n",
    "        \n",
    "        feed_inputs, feed_targets = batch_generator.next()\n",
    "        feed_dict = {inputs : feed_inputs, targets : feed_targets}\n",
    "        if controller_final_state_autoregressive_cur is not None:\n",
    "            feed_dict.update({controller_initial_state_autoregressive : controller_final_state_autoregressive_cur})\n",
    "        if controller_final_state_gt_cur is not None:\n",
    "            feed_dict.update({controller_final_state_gt : controller_final_state_gt_cur})\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            feed_dict.update({keep_prob : KEEP_PROB_TRAIN})\n",
    "            summary, _, loss, controller_final_state_gt_cur, controller_final_state_autoregressive_cur = \\\n",
    "                session.run([summaries, optimizer, mse_autoregressive_steering, controller_final_state_gt, controller_final_state_autoregressive],\n",
    "                           feed_dict = feed_dict)\n",
    "            train_writer.add_summary(summary, global_train_step)\n",
    "            global_train_step += 1\n",
    "        \n",
    "        elif mode == \"valid\":\n",
    "            model_predictions, summary, loss, controller_final_state_autoregressive_cur = \\\n",
    "                session.run([steering_predictions, summaries, mse_autoregressive_steering, controller_final_state_autoregressive],\n",
    "                           feed_dict = feed_dict)\n",
    "            valid_writer.add_summary(summary, global_valid_step)\n",
    "            global_valid_step += 1\n",
    "            \n",
    "            feed_inputs = feed_inputs[:, LEFT_CONTEXT:].flatten()\n",
    "            steering_targets = feed_targets[:, :, 0].flatten()\n",
    "            model_predictions = model_predictions.flatten()\n",
    "            stats = np.stack([steering_targets, model_predictions, (steering_targets - model_predictions)**2])\n",
    "            for i, img in enumerate(feed_inputs):\n",
    "                global_valid_predictions[img] = stats[:, i]\n",
    "        \n",
    "        elif mode == \"test\":\n",
    "            model_predictions, controller_final_state_autoregressive_cur = \\\n",
    "                session.run([steering_predictions, controller_final_state_autoregressive],\n",
    "                           feed_dict = feed_dict)           \n",
    "            feed_inputs = feed_inputs[:, LEFT_CONTEXT:].flatten()\n",
    "            model_predictions = model_predictions.flatten()\n",
    "            for i, img in enumerate(feed_inputs):\n",
    "                test_predictions[img] = model_predictions[i]\n",
    "        \n",
    "        if mode != \"test\":\n",
    "            acc_loss += loss\n",
    "            print('\\r', step + 1, \"/\", total_num_steps, np.sqrt(acc_loss / (step+1)))\n",
    "\n",
    "    return np.sqrt(acc_loss / total_num_steps) if mode != \"test\" else test_predictions\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS=10\n",
    "\n",
    "best_validation_score = None\n",
    "\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options)) as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    print('Initialized')\n",
    "    ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    if ckpt:\n",
    "        print(\"Restoring from\", ckpt)\n",
    "        saver.restore(sess=session, save_path=ckpt)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"Starting epoch %d\" % epoch)\n",
    "        print(\"Validation:\")\n",
    "        valid_score = do_epoch(session=session, sequences=valid_seq, mode=\"valid\")\n",
    "        if best_validation_score is None: \n",
    "            best_validation_score = valid_score\n",
    "            with open(\"v3/test-predictions-epoch%d\" % epoch, \"w\") as out:\n",
    "                test_predictions = do_epoch(session=session, sequences=test_seq, mode=\"test\")\n",
    "                # print >> out, \"frame_id,steering_angle\"\n",
    "                for img, pred in test_predictions.items():\n",
    "                    img = img.replace(\"challenge_2/Test-final/center/\", \"\")\n",
    "                    # print >> out, \"%s,%f\" % (img, pred)\n",
    "        \n",
    "        if valid_score < best_validation_score:\n",
    "            saver.save(session, 'v3/checkpoint-sdc-ch2')\n",
    "            best_validation_score = valid_score\n",
    "            print(\"SAVED at epoch %d\" % epoch) \n",
    "            with open(\"v3/valid-predictions-epoch%d\" % epoch, \"w\") as out:\n",
    "                result = np.float128(0.0)\n",
    "                for img, stats in global_valid_predictions.items():\n",
    "                    # print >> out, img, stats\n",
    "                    result += stats[-1]\n",
    "            print(\"Validation unnormalized RMSE:\", np.sqrt(result / len(global_valid_predictions)))\n",
    "            with open(\"v3/test-predictions-epoch%d\" % epoch, \"w\") as out:\n",
    "                test_predictions = do_epoch(session=session, sequences=test_seq, mode=\"test\")\n",
    "                # print >> out, \"frame_id,steering_angle\"\n",
    "                for img, pred in test_predictions.items():\n",
    "                    img = img.replace(\"challenge_2/Test-final/center/\", \"\")\n",
    "                    # print >> out, \"%s,%f\" % (img, pred)\n",
    "        if epoch != NUM_EPOCHS - 1:\n",
    "            print(\"Training\")\n",
    "            do_epoch(session=session, sequences=train_seq, mode=\"train\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
